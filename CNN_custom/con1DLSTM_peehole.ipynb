{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f180f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5511476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Data ----------\n",
    "def generate_time_series_data(samples, timesteps, features):\n",
    "    X = np.random.rand(samples, timesteps, features).astype(np.float32)\n",
    "    y = np.sum(X, axis=2)[:, -1].reshape(-1, 1)  # target = sum of last timestep\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "\n",
    "X, y = generate_time_series_data(samples=1000, timesteps=10, features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ac4c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8669, 0.7431],\n",
       "         [0.1735, 0.6549],\n",
       "         [0.6133, 0.0882],\n",
       "         ...,\n",
       "         [0.2493, 0.8621],\n",
       "         [0.5048, 0.7832],\n",
       "         [0.1657, 0.5222]],\n",
       "\n",
       "        [[0.6127, 0.6289],\n",
       "         [0.8480, 0.1342],\n",
       "         [0.6371, 0.0681],\n",
       "         ...,\n",
       "         [0.7495, 0.9593],\n",
       "         [0.1693, 0.4477],\n",
       "         [0.4741, 0.2324]],\n",
       "\n",
       "        [[0.8222, 0.1830],\n",
       "         [0.2762, 0.5665],\n",
       "         [0.8521, 0.1222],\n",
       "         ...,\n",
       "         [0.7275, 0.9126],\n",
       "         [0.7390, 0.2893],\n",
       "         [0.3775, 0.8459]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8739, 0.6744],\n",
       "         [0.2480, 0.8189],\n",
       "         [0.4200, 0.6663],\n",
       "         ...,\n",
       "         [0.7915, 0.7773],\n",
       "         [0.5981, 0.0674],\n",
       "         [0.5378, 0.2118]],\n",
       "\n",
       "        [[0.4561, 0.8353],\n",
       "         [0.4427, 0.7039],\n",
       "         [0.9481, 0.1668],\n",
       "         ...,\n",
       "         [0.6151, 0.9097],\n",
       "         [0.8367, 0.8171],\n",
       "         [0.1066, 0.0636]],\n",
       "\n",
       "        [[0.1010, 0.0231],\n",
       "         [0.2905, 0.5338],\n",
       "         [0.0864, 0.3666],\n",
       "         ...,\n",
       "         [0.2079, 0.7527],\n",
       "         [0.6950, 0.4008],\n",
       "         [0.4470, 0.3043]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Model ----------\n",
    "class ConvLSTM1DCell(nn.Module):\n",
    "    def __init__(self, input_dim, filters, kernel_size, padding='same'):\n",
    "        super(ConvLSTM1DCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2 if padding == 'same' else 0\n",
    "\n",
    "        self.conv_f = nn.Conv1d(input_dim + filters, filters, kernel_size, padding=self.padding)\n",
    "        self.conv_i = nn.Conv1d(input_dim + filters, filters, kernel_size, padding=self.padding)\n",
    "        self.conv_c = nn.Conv1d(input_dim + filters, filters, kernel_size, padding=self.padding)\n",
    "        self.conv_o = nn.Conv1d(input_dim + filters, filters, kernel_size, padding=self.padding)\n",
    "\n",
    "        self.peephole_f = nn.Conv1d(filters, filters, kernel_size=1)\n",
    "        self.peephole_i = nn.Conv1d(filters, filters, kernel_size=1)\n",
    "        self.peephole_o = nn.Conv1d(filters, filters, kernel_size=1)\n",
    "\n",
    "    def forward(self, x_t, states):\n",
    "        h_prev, c_prev = states\n",
    "        combined = torch.cat([x_t, h_prev], dim=1)\n",
    "        f_t = torch.sigmoid(self.conv_f(combined) + self.peephole_f(c_prev))\n",
    "        i_t = torch.sigmoid(self.conv_i(combined) + self.peephole_i(c_prev))\n",
    "        c_tilde = torch.tanh(self.conv_c(combined))\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        o_t = torch.sigmoid(self.conv_o(combined) + self.peephole_o(c_t))\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, (h_t, c_t)\n",
    "\n",
    "class ConvLSTM1D(nn.Module):\n",
    "    def __init__(self, input_dim, filters, kernel_size, return_sequences=False, padding='same'):\n",
    "        super(ConvLSTM1D, self).__init__()\n",
    "        self.cell = ConvLSTM1DCell(input_dim, filters, kernel_size, padding)\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
    "        batch_size, input_dim, seq_len = x.size()\n",
    "\n",
    "        h_t = torch.zeros(batch_size, self.cell.filters, 1, device=x.device)\n",
    "        c_t = torch.zeros(batch_size, self.cell.filters, 1, device=x.device)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, :, t:t+1]  # (batch, features, 1)\n",
    "            h_t, (h_t, c_t) = self.cell(x_t, (h_t, c_t))\n",
    "            outputs.append(h_t)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, filters, seq_len)\n",
    "        if not self.return_sequences:\n",
    "            outputs = outputs[:, -1, :, :]  # (batch, filters, seq_len)\n",
    "        return outputs, (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a892bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training ----------\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.convlstm = ConvLSTM1D(input_dim, filters, kernel_size)\n",
    "        self.fc = nn.Linear(filters, 1)  # Final regression layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.convlstm(x)\n",
    "        out = out.mean(dim=2)  # Global average pooling over sequence length\n",
    "        return self.fc(out)\n",
    "\n",
    "model = ConvLSTMModel(input_dim=8, filters=16, kernel_size=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba31332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "X, y = X.to(device), y.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14513e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 10 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      4\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m loss = criterion(output, y)\n\u001b[32m      7\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mConvLSTMModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     out = out.mean(dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# Global average pooling over sequence length\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mConvLSTM1D.forward\u001b[39m\u001b[34m(self, x, hidden_state)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[32m     45\u001b[39m     x_t = x[:, :, t:t+\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# (batch, features, 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     h_t, (h_t, c_t) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     outputs.append(h_t)\n\u001b[32m     49\u001b[39m outputs = torch.stack(outputs, dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch, seq_len, filters, seq_len)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mini\\envs\\pine\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mConvLSTM1DCell.forward\u001b[39m\u001b[34m(self, x_t, states)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_t, states):\n\u001b[32m     20\u001b[39m     h_prev, c_prev = states\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     combined = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     f_t = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.conv_f(combined) + \u001b[38;5;28mself\u001b[39m.peephole_f(c_prev))\n\u001b[32m     23\u001b[39m     i_t = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.conv_i(combined) + \u001b[38;5;28mself\u001b[39m.peephole_i(c_prev))\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 10 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# ---------- Training Loop ----------\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch:02d} - Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
